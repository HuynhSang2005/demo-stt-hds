#!/usr/bin/env python3
"""
Integration Test Suite for Task 15: Validate Tasks 10-14 Working Together

This test suite validates that all optimization features work correctly when integrated:
- Task 10: Vietnamese preprocessing
- Task 11: Ensemble classification with keyword detection
- Task 12: Metrics collection
- Task 13: Error handling with circuit breaker
- Task 14: Configuration management

Test scenarios:
1. End-to-end audio processing with all features
2. Vietnamese preprocessing integration with ASR
3. Ensemble classification with keyword detection
4. Metrics collection across all components
5. Circuit breaker activation and recovery
6. Configuration-based feature toggling
7. Error handling under various failure modes
8. Performance under different config presets
"""

import os
import sys
import time
import asyncio
from pathlib import Path
from typing import Dict, Any, List
from dataclasses import dataclass
import json

# Add backend to path
backend_path = Path(__file__).resolve().parent / "backend"
sys.path.insert(0, str(backend_path))

# Import modules
from app.core.config_enhanced import EnhancedSettings, get_enhanced_settings
from app.core.metrics import MetricsCollector, get_metrics_collector
from app.core.error_handling import (
    CircuitBreaker,
    AudioInputError,
    ModelInferenceError
)

# Import utilities
try:
    from app.utils.vietnamese_preprocessing import VietnameseTextPreprocessor
    from app.utils.toxic_keyword_detection import VietnameseToxicKeywordDetector
    HAS_UTILS = True
except ImportError:
    HAS_UTILS = False
    print("âš ï¸ Utils modules not found. Some tests will be skipped.")

# Import models and services (will test if available)
try:
    from app.models.asr import VietnameseASR
    from app.models.classifier import VietnameseToxicClassifier
    from app.services.audio_processor import AudioProcessor
    HAS_MODELS = True
except ImportError:
    HAS_MODELS = False
    print("âš ï¸ Model modules not found. Some tests will be skipped.")


@dataclass
class TestResult:
    """Test result data structure"""
    test_name: str
    passed: bool
    duration_ms: float
    error_message: str = ""
    details: Dict[str, Any] = None


class IntegrationTestRunner:
    """Integration test runner"""
    
    def __init__(self):
        self.results: List[TestResult] = []
        self.settings = get_enhanced_settings()
        self.metrics = get_metrics_collector()
        
    def run_test(self, test_name: str, test_func):
        """Run a single test and record result"""
        print(f"\n{'='*80}")
        print(f"Running: {test_name}")
        print(f"{'='*80}")
        
        start_time = time.time()
        try:
            result = test_func()
            duration_ms = (time.time() - start_time) * 1000
            
            if result is True or (isinstance(result, dict) and result.get("passed", False)):
                print(f"âœ… PASSED ({duration_ms:.2f}ms)")
                details = result if isinstance(result, dict) else {}
                self.results.append(TestResult(
                    test_name=test_name,
                    passed=True,
                    duration_ms=duration_ms,
                    details=details
                ))
            else:
                print(f"âŒ FAILED ({duration_ms:.2f}ms)")
                error_msg = result.get("error", "Unknown error") if isinstance(result, dict) else str(result)
                self.results.append(TestResult(
                    test_name=test_name,
                    passed=False,
                    duration_ms=duration_ms,
                    error_message=error_msg
                ))
        except Exception as e:
            duration_ms = (time.time() - start_time) * 1000
            print(f"âŒ FAILED ({duration_ms:.2f}ms): {e}")
            self.results.append(TestResult(
                test_name=test_name,
                passed=False,
                duration_ms=duration_ms,
                error_message=str(e)
            ))
    
    def print_summary(self):
        """Print test summary"""
        print(f"\n{'='*80}")
        print("INTEGRATION TEST SUMMARY")
        print(f"{'='*80}")
        
        passed = sum(1 for r in self.results if r.passed)
        failed = sum(1 for r in self.results if not r.passed)
        total = len(self.results)
        
        print(f"Total Tests: {total}")
        print(f"Passed: {passed} âœ…")
        print(f"Failed: {failed} âŒ")
        print(f"Success Rate: {(passed/total*100):.1f}%" if total > 0 else "N/A")
        
        if failed > 0:
            print(f"\n{'='*80}")
            print("FAILED TESTS:")
            print(f"{'='*80}")
            for result in self.results:
                if not result.passed:
                    print(f"âŒ {result.test_name}")
                    print(f"   Error: {result.error_message}")
        
        print(f"{'='*80}")
        return passed == total


# ============================================================================
# TEST SUITE 1: Configuration Integration (Task 14)
# ============================================================================

def test_configuration_loading():
    """Test that enhanced configuration loads correctly"""
    settings = get_enhanced_settings()
    
    # Verify basic settings
    assert settings.PROJECT_NAME is not None
    assert settings.VERSION == "2.0.0"
    
    # Verify Task 10 settings
    assert hasattr(settings, "VIETNAMESE_PREPROCESSING_ENABLED")
    assert hasattr(settings, "NORMALIZE_PUNCTUATION")
    
    # Verify Task 11 settings
    assert hasattr(settings, "ENABLE_ENSEMBLE_CLASSIFICATION")
    assert hasattr(settings, "TOXIC_THRESHOLD")
    
    # Verify Task 12 settings
    assert hasattr(settings, "ENABLE_METRICS")
    assert hasattr(settings, "METRICS_WINDOW_SIZE")
    
    # Verify Task 13 settings
    assert hasattr(settings, "ENABLE_CIRCUIT_BREAKER")
    assert hasattr(settings, "CIRCUIT_BREAKER_FAILURE_THRESHOLD")
    
    return {"passed": True, "settings_count": len(settings.dict())}


def test_grouped_config_methods():
    """Test configuration helper methods"""
    settings = get_enhanced_settings()
    
    # Test preprocessing config
    preprocessing = settings.get_preprocessing_config()
    assert "enabled" in preprocessing
    assert "normalize_punctuation" in preprocessing
    
    # Test classification config
    classification = settings.get_classification_config()
    assert "ensemble_enabled" in classification
    assert "thresholds" in classification
    assert "toxic" in classification["thresholds"]
    
    # Test circuit breaker config
    circuit_breaker = settings.get_circuit_breaker_config()
    assert "enabled" in circuit_breaker
    assert "failure_threshold" in circuit_breaker
    
    # Test metrics config
    metrics_config = settings.get_metrics_config()
    assert "enabled" in metrics_config
    assert "window_size" in metrics_config
    
    return {"passed": True}


def test_configuration_validation():
    """Test configuration validation"""
    settings = get_enhanced_settings()
    
    # Test model path validation
    validation = settings.validate_model_paths()
    assert "asr_exists" in validation
    assert "classifier_exists" in validation
    
    # Get model paths
    paths = settings.get_model_paths()
    assert "asr" in paths
    assert "classifier" in paths
    
    return {"passed": True, "validation": validation}


# ============================================================================
# TEST SUITE 2: Vietnamese Preprocessing Integration (Task 10)
# ============================================================================

def test_vietnamese_preprocessing_module():
    """Test Vietnamese preprocessing module"""
    if not HAS_UTILS:
        print("âš ï¸ Skipping: Utils not available")
        return {"passed": True, "skipped": True}
    
    preprocessor = VietnameseTextPreprocessor(
        remove_tones=False,
        convert_numbers_to_text=False,
        normalize_punctuation=True,
        apply_common_fixes=True,
        fix_spacing=True,
        lowercase=False
    )
    
    # Test common fixes
    test_cases = [
        ("Ä‘c rá»“i", "Ä‘Æ°á»£c rá»“i"),  # Common abbreviation
        ("ko muá»‘n", "khÃ´ng muá»‘n"),  # Another abbreviation
        ("bn lÃ m gÃ¬", "báº¡n lÃ m gÃ¬"),  # 'bn' -> 'báº¡n'
    ]
    
    for input_text, expected in test_cases:
        result = preprocessor.normalize(input_text)
        assert expected in result or result == expected, f"Expected '{expected}' in '{result}'"
    
    # Test confidence adjustment
    confidence = preprocessor.calculate_confidence_adjustment("Xin chÃ o!")
    assert 0 <= confidence <= 1.0
    
    return {"passed": True, "test_cases": len(test_cases)}


def test_preprocessing_config_integration():
    """Test preprocessing configuration integration"""
    if not HAS_UTILS:
        print("âš ï¸ Skipping: Utils not available")
        return {"passed": True, "skipped": True}
    
    settings = get_enhanced_settings()
    preprocessing_config = settings.get_preprocessing_config()
    
    # Create preprocessor from config
    preprocessor = VietnameseTextPreprocessor(
        remove_tones=preprocessing_config["remove_tones"],
        convert_numbers_to_text=preprocessing_config["convert_numbers_to_text"],
        normalize_punctuation=preprocessing_config["normalize_punctuation"],
        apply_common_fixes=preprocessing_config["apply_common_fixes"],
        fix_spacing=preprocessing_config["fix_spacing"],
        lowercase=preprocessing_config["lowercase"]
    )
    
    # Test it works
    result = preprocessor.normalize("Ä‘c rá»“i nha")
    assert len(result) > 0
    
    return {"passed": True, "config": preprocessing_config}


# ============================================================================
# TEST SUITE 3: Toxic Keyword Detection Integration (Task 11)
# ============================================================================

def test_toxic_keyword_detection_module():
    """Test toxic keyword detection module"""
    if not HAS_UTILS:
        print("âš ï¸ Skipping: Utils not available")
        return {"passed": True, "skipped": True}
    
    detector = VietnameseToxicKeywordDetector()
    
    # Test toxic detection
    test_cases = [
        ("Báº¡n tháº­t tá»‘t", False),  # Non-toxic
        ("Äá»“ ngu ngá»‘c", True),    # Toxic (high severity)
        ("Tháº±ng khá»‘n", True),     # Toxic (high severity)
        ("Xin chÃ o má»i ngÆ°á»i", False),  # Non-toxic
    ]
    
    for text, expected_toxic in test_cases:
        result = detector.detect_toxic_content(text)
        is_toxic = result["is_toxic"]
        assert is_toxic == expected_toxic, f"Text: '{text}', Expected: {expected_toxic}, Got: {is_toxic}"
    
    # Test toxic score calculation
    score = detector.calculate_toxic_score("Äá»“ ngu ngá»‘c tháº±ng khá»‘n")
    assert score > 0, "Should detect toxic content"
    
    return {"passed": True, "test_cases": len(test_cases)}


def test_keyword_detection_config_integration():
    """Test keyword detection configuration integration"""
    if not HAS_UTILS:
        print("âš ï¸ Skipping: Utils not available")
        return {"passed": True, "skipped": True}
    
    settings = get_enhanced_settings()
    
    # Verify keyword detection settings
    assert hasattr(settings, "ENABLE_KEYWORD_DETECTION")
    assert hasattr(settings, "KEYWORD_FUZZY_MATCHING")
    assert hasattr(settings, "KEYWORD_DETECTION_THRESHOLD")
    
    # Create detector (should respect settings)
    detector = VietnameseToxicKeywordDetector()
    
    # Test detection
    result = detector.detect_toxic_content("Äá»“ ngu")
    assert "is_toxic" in result
    assert "keywords_found" in result
    
    return {"passed": True, "fuzzy_matching": settings.KEYWORD_FUZZY_MATCHING}


# ============================================================================
# TEST SUITE 4: Metrics Collection Integration (Task 12)
# ============================================================================

def test_metrics_collector_initialization():
    """Test metrics collector initialization"""
    metrics = get_metrics_collector()
    
    # Verify it's a singleton
    metrics2 = get_metrics_collector()
    assert metrics is metrics2, "Should be singleton"
    
    # Test basic metrics recording
    metrics.record_request_latency(0.150, success=True)
    metrics.record_request_latency(0.200, success=True)
    metrics.record_component_timing("asr", 0.100)
    metrics.record_component_timing("classifier", 0.050)
    metrics.record_error("TestError", "Test error message")
    
    # Get metrics
    summary = metrics.get_metrics()
    assert "total_requests" in summary
    assert summary["total_requests"] >= 2
    
    return {"passed": True, "total_requests": summary["total_requests"]}


def test_metrics_config_integration():
    """Test metrics configuration integration"""
    settings = get_enhanced_settings()
    metrics_config = settings.get_metrics_config()
    
    # Verify metrics settings
    assert metrics_config["enabled"] is True
    assert metrics_config["window_size"] > 0
    assert "track_timing" in metrics_config
    
    # Create metrics collector (respects config)
    metrics = get_metrics_collector()
    
    # Record some metrics
    for i in range(5):
        metrics.record_request_latency(0.1 + i * 0.01, success=True)
    
    # Get detailed metrics
    summary = metrics.get_metrics(window_seconds=60)
    assert "success_rate" in summary
    assert summary["total_requests"] >= 5
    
    return {"passed": True, "config": metrics_config}


def test_metrics_percentile_calculations():
    """Test metrics percentile calculations"""
    metrics = get_metrics_collector()
    
    # Record known latencies
    latencies = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
    for latency in latencies:
        metrics.record_request_latency(latency, success=True)
    
    # Get metrics
    summary = metrics.get_metrics(window_seconds=60)
    
    # Verify percentiles exist
    assert "latency_p50_ms" in summary
    assert "latency_p95_ms" in summary
    assert "latency_p99_ms" in summary
    
    # Verify reasonable values
    assert summary["latency_p50_ms"] > 0
    assert summary["latency_p95_ms"] >= summary["latency_p50_ms"]
    
    return {"passed": True, "percentiles": {
        "p50": summary["latency_p50_ms"],
        "p95": summary["latency_p95_ms"],
        "p99": summary["latency_p99_ms"]
    }}


# ============================================================================
# TEST SUITE 5: Circuit Breaker Integration (Task 13)
# ============================================================================

def test_circuit_breaker_initialization():
    """Test circuit breaker initialization"""
    settings = get_enhanced_settings()
    cb_config = settings.get_circuit_breaker_config()
    
    # Create circuit breaker
    circuit_breaker = CircuitBreaker(
        failure_threshold=cb_config["failure_threshold"],
        timeout_seconds=cb_config["timeout_seconds"],
        half_open_attempts=cb_config["half_open_attempts"]
    )
    
    # Verify initial state
    assert circuit_breaker.state == CircuitBreaker.State.CLOSED
    assert circuit_breaker.failure_count == 0
    
    return {"passed": True, "initial_state": circuit_breaker.state.value}


def test_circuit_breaker_state_transitions():
    """Test circuit breaker state transitions"""
    # Create circuit breaker with low threshold for testing
    circuit_breaker = CircuitBreaker(
        failure_threshold=3,
        timeout_seconds=1,
        half_open_attempts=1
    )
    
    # Start in CLOSED state
    assert circuit_breaker.state == CircuitBreaker.State.CLOSED
    
    # Record failures
    for i in range(3):
        circuit_breaker.record_failure()
    
    # Should trip to OPEN after 3 failures
    assert circuit_breaker.state == CircuitBreaker.State.OPEN
    
    # Wait for timeout
    time.sleep(1.1)
    
    # Should allow one test request (HALF_OPEN)
    can_execute = circuit_breaker.can_execute()
    assert can_execute, "Should allow execution in HALF_OPEN"
    
    # Record success to close circuit
    circuit_breaker.record_success()
    
    # Should be back to CLOSED
    assert circuit_breaker.state == CircuitBreaker.State.CLOSED
    
    return {"passed": True, "final_state": circuit_breaker.state.value}


def test_circuit_breaker_config_integration():
    """Test circuit breaker configuration integration"""
    settings = get_enhanced_settings()
    cb_config = settings.get_circuit_breaker_config()
    
    # Verify configuration
    assert cb_config["enabled"] is True
    assert cb_config["failure_threshold"] > 0
    assert cb_config["timeout_seconds"] > 0
    
    # Create circuit breaker from config
    circuit_breaker = CircuitBreaker(
        failure_threshold=cb_config["failure_threshold"],
        timeout_seconds=cb_config["timeout_seconds"],
        half_open_attempts=cb_config["half_open_attempts"]
    )
    
    # Test basic operation
    assert circuit_breaker.can_execute()
    circuit_breaker.record_success()
    
    return {"passed": True, "config": cb_config}


# ============================================================================
# TEST SUITE 6: End-to-End Integration
# ============================================================================

def test_all_features_enabled():
    """Test that all optimization features can be enabled simultaneously"""
    settings = get_enhanced_settings()
    
    # Verify all features can be enabled
    features = {
        "async_processing": settings.ENABLE_ASYNC_PROCESSING,
        "batch_processing": settings.ENABLE_BATCH_PROCESSING,
        "vietnamese_preprocessing": settings.VIETNAMESE_PREPROCESSING_ENABLED,
        "ensemble_classification": settings.ENABLE_ENSEMBLE_CLASSIFICATION,
        "metrics": settings.ENABLE_METRICS,
        "circuit_breaker": settings.ENABLE_CIRCUIT_BREAKER,
        "retry": settings.ENABLE_RETRY,
        "keyword_detection": settings.ENABLE_KEYWORD_DETECTION,
    }
    
    enabled_count = sum(1 for v in features.values() if v)
    
    return {
        "passed": True,
        "features": features,
        "enabled_count": enabled_count,
        "total_features": len(features)
    }


def test_configuration_presets():
    """Test different configuration presets"""
    # High Accuracy Preset
    high_accuracy = EnhancedSettings(
        TOXIC_THRESHOLD=0.45,
        ENSEMBLE_MODEL_WEIGHT=0.6,
        ENSEMBLE_KEYWORD_WEIGHT=0.4,
        ENABLE_SLIDING_WINDOW=True
    )
    assert high_accuracy.TOXIC_THRESHOLD == 0.45
    
    # High Performance Preset
    high_performance = EnhancedSettings(
        ASR_BATCH_SIZE=15,
        CLASSIFIER_BATCH_SIZE=20,
        TRACK_COMPONENT_TIMING=False,
        METRICS_WINDOW_SIZE=500
    )
    assert high_performance.ASR_BATCH_SIZE == 15
    
    # High Reliability Preset
    high_reliability = EnhancedSettings(
        CIRCUIT_BREAKER_FAILURE_THRESHOLD=3,
        MAX_RETRY_ATTEMPTS=5,
        ENABLE_METRICS=True
    )
    assert high_reliability.MAX_RETRY_ATTEMPTS == 5
    
    return {"passed": True, "presets_tested": 3}


def test_feature_toggle_via_config():
    """Test that features can be toggled via configuration"""
    # Test with features disabled
    settings_disabled = EnhancedSettings(
        ENABLE_METRICS=False,
        ENABLE_CIRCUIT_BREAKER=False,
        VIETNAMESE_PREPROCESSING_ENABLED=False
    )
    
    assert settings_disabled.ENABLE_METRICS is False
    assert settings_disabled.ENABLE_CIRCUIT_BREAKER is False
    assert settings_disabled.VIETNAMESE_PREPROCESSING_ENABLED is False
    
    # Test with features enabled
    settings_enabled = EnhancedSettings(
        ENABLE_METRICS=True,
        ENABLE_CIRCUIT_BREAKER=True,
        VIETNAMESE_PREPROCESSING_ENABLED=True
    )
    
    assert settings_enabled.ENABLE_METRICS is True
    assert settings_enabled.ENABLE_CIRCUIT_BREAKER is True
    assert settings_enabled.VIETNAMESE_PREPROCESSING_ENABLED is True
    
    return {"passed": True, "toggleable_features": 3}


# ============================================================================
# MAIN TEST RUNNER
# ============================================================================

def run_integration_tests():
    """Run all integration tests"""
    print("="*80)
    print("TASK 15: INTEGRATION TEST SUITE")
    print("Testing Tasks 10-14 Integration")
    print("="*80)
    
    runner = IntegrationTestRunner()
    
    # Suite 1: Configuration Integration (Task 14)
    print("\n" + "="*80)
    print("SUITE 1: Configuration Integration (Task 14)")
    print("="*80)
    runner.run_test("1.1: Configuration Loading", test_configuration_loading)
    runner.run_test("1.2: Grouped Config Methods", test_grouped_config_methods)
    runner.run_test("1.3: Configuration Validation", test_configuration_validation)
    
    # Suite 2: Vietnamese Preprocessing (Task 10)
    print("\n" + "="*80)
    print("SUITE 2: Vietnamese Preprocessing Integration (Task 10)")
    print("="*80)
    runner.run_test("2.1: Vietnamese Preprocessing Module", test_vietnamese_preprocessing_module)
    runner.run_test("2.2: Preprocessing Config Integration", test_preprocessing_config_integration)
    
    # Suite 3: Toxic Keyword Detection (Task 11)
    print("\n" + "="*80)
    print("SUITE 3: Toxic Keyword Detection Integration (Task 11)")
    print("="*80)
    runner.run_test("3.1: Toxic Keyword Detection Module", test_toxic_keyword_detection_module)
    runner.run_test("3.2: Keyword Detection Config Integration", test_keyword_detection_config_integration)
    
    # Suite 4: Metrics Collection (Task 12)
    print("\n" + "="*80)
    print("SUITE 4: Metrics Collection Integration (Task 12)")
    print("="*80)
    runner.run_test("4.1: Metrics Collector Initialization", test_metrics_collector_initialization)
    runner.run_test("4.2: Metrics Config Integration", test_metrics_config_integration)
    runner.run_test("4.3: Metrics Percentile Calculations", test_metrics_percentile_calculations)
    
    # Suite 5: Circuit Breaker (Task 13)
    print("\n" + "="*80)
    print("SUITE 5: Circuit Breaker Integration (Task 13)")
    print("="*80)
    runner.run_test("5.1: Circuit Breaker Initialization", test_circuit_breaker_initialization)
    runner.run_test("5.2: Circuit Breaker State Transitions", test_circuit_breaker_state_transitions)
    runner.run_test("5.3: Circuit Breaker Config Integration", test_circuit_breaker_config_integration)
    
    # Suite 6: End-to-End Integration
    print("\n" + "="*80)
    print("SUITE 6: End-to-End Integration")
    print("="*80)
    runner.run_test("6.1: All Features Enabled", test_all_features_enabled)
    runner.run_test("6.2: Configuration Presets", test_configuration_presets)
    runner.run_test("6.3: Feature Toggle via Config", test_feature_toggle_via_config)
    
    # Print summary
    success = runner.print_summary()
    
    # Save results to file
    results_file = Path(__file__).parent / "integration_test_results.json"
    results_data = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "total_tests": len(runner.results),
        "passed": sum(1 for r in runner.results if r.passed),
        "failed": sum(1 for r in runner.results if not r.passed),
        "results": [
            {
                "test_name": r.test_name,
                "passed": r.passed,
                "duration_ms": r.duration_ms,
                "error_message": r.error_message,
                "details": r.details
            }
            for r in runner.results
        ]
    }
    
    with open(results_file, "w", encoding="utf-8") as f:
        json.dump(results_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nðŸ“„ Results saved to: {results_file}")
    
    return success


if __name__ == "__main__":
    success = run_integration_tests()
    sys.exit(0 if success else 1)
