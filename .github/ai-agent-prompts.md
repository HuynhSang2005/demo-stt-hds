# AI Agent Task Prompts ‚Äì Offline-First Speech-to-Text + Toxic Detection (Vietnamese)

> **M·ª•c ti√™u**: X√¢y d·ª±ng webapp local, **offline**, **real-time**, d√πng **2 m√¥ h√¨nh ƒë√£ clone s·∫µn**:
> - `./models/wav2vec2-base-vietnamese-250h/`
> - `./models/phobert-vi-comment-4class/`
> 
> **Y√™u c·∫ßu chung**:
> - Kh√¥ng g·ªçi Hugging Face Hub (offline-only)
> - 
> - Real-time qua WebSocket (chunk ~2s)
> - Ch·ªâ c·∫£nh b√°o khi label = `"toxic"` ho·∫∑c `"negative"`

---

## üîπ GIAI ƒêO·∫†N 1: X√ÅC MINH MODEL LOCAL

### ‚úÖ Prompt 1.1 ‚Äì Ki·ªÉm tra t√≠nh to√†n v·∫πn model
> "Ki·ªÉm tra folder `./models/wav2vec2-base-vietnamese-250h/` c√≥ ƒë·ªß c√°c file sau:
> - `config.json`
> - `pytorch_model.bin`
> - `processor_config.json`
> - `vocab.json`
> - `tokenizer_config.json`
> 
> T∆∞∆°ng t·ª±, ki·ªÉm tra `./models/phobert-vi-comment-4class/` c√≥:
> - `config.json`
> - `pytorch_model.bin`
> - `tokenizer_config.json`
> - `special_tokens_map.json`
> 
> N·∫øu thi·∫øu file n√†o ‚Üí b√°o l·ªói r√µ r√†ng."

### ‚úÖ Prompt 1.2 ‚Äì X√°c minh load model offline
> "Vi·∫øt script Python `verify_models.py`:
> - D√πng `Wav2Vec2Processor.from_pretrained(local_path, local_files_only=True)`
> - D√πng `Wav2Vec2ForCTC.from_pretrained(...)` t∆∞∆°ng t·ª±
> - D√πng `pipeline('text-classification', model=..., local_files_only=True)`
> - In ra: `'‚úÖ ASR loaded'`, `'‚úÖ Classifier loaded'`
> - N·∫øu l·ªói ‚Üí in traceback v√† d·ª´ng"

---

## üîπ GIAI ƒêO·∫†N 2: BACKEND ‚Äì OFFLINE INFERENCE & FASTAPI SETUP

### ‚úÖ Prompt 2.1 ‚Äì C·∫•u h√¨nh d·ª± √°n FastAPI
> "T·∫°o th∆∞ m·ª•c `backend/` v·ªõi c·∫•u tr√∫c:
> ```
> backend/
> ‚îú‚îÄ‚îÄ app/
> ‚îÇ   ‚îú‚îÄ‚îÄ main.py
> ‚îÇ   ‚îú‚îÄ‚îÄ core/
> ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
> ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py        # PydanticSettings: model_paths
> ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logger.py        # JSON structured logger
> ‚îÇ   ‚îú‚îÄ‚îÄ models/
> ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
> ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ asr.py           # LocalWav2Vec2ASR
> ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ classifier.py    # LocalPhoBERTClassifier
> ‚îÇ   ‚îú‚îÄ‚îÄ schemas/
> ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ audio.py         # Pydantic: TranscriptResult
> ‚îÇ   ‚îú‚îÄ‚îÄ services/
> ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ audio_processor.py  # resample + pipeline
> ‚îÇ   ‚îî‚îÄ‚îÄ api/
> ‚îÇ       ‚îî‚îÄ‚îÄ v1/
> ‚îÇ           ‚îî‚îÄ‚îÄ endpoints.py # WebSocket route
> ‚îú‚îÄ‚îÄ requirements.txt
> ‚îî‚îÄ‚îÄ README.md
> ```
> - `requirements.txt` ph·∫£i c√≥: `fastapi`, `uvicorn`, `torch`, `transformers`, `torchaudio`, `onnxruntime` (t√πy ch·ªçn)
> - `config.py`: ƒë·ªãnh nghƒ©a `ASR_MODEL_PATH = './models/wav2vec2-base-vietnamese-250h'`, `CLASSIFIER_MODEL_PATH = './models/phobert-vi-comment-4class'`"

### ‚úÖ Prompt 2.2 ‚Äì Tri·ªÉn khai ASR model t·ª´ local
> "Trong `backend/app/models/asr.py`, vi·∫øt class `LocalWav2Vec2ASR`:
> - `__init__(self, model_path: str)`: load processor + model v·ªõi `local_files_only=True`
> - `transcribe(self, audio_bytes: bytes) -> str`:
>   1. D√πng `io.BytesIO(audio_bytes)` ‚Üí `torchaudio.load()`
>   2. N·∫øu sample rate ‚â† 16000 ‚Üí resample b·∫±ng `torchaudio.functional.resample`
>   3. ƒê·∫£m b·∫£o waveform shape = `(1, T)`
>   4. D√πng processor ‚Üí input_values ‚Üí model ‚Üí argmax ‚Üí decode
>   5. Tr·∫£ v·ªÅ text (str), kh√¥ng c√≥ d·∫•u ngo·∫∑c, kh√¥ng log
> - X·ª≠ l√Ω exception ‚Üí tr·∫£ `'[ASR_ERROR]'`"

### ‚úÖ Prompt 2.3 ‚Äì Tri·ªÉn khai classifier t·ª´ local
> "Trong `backend/app/models/classifier.py`, vi·∫øt class `LocalPhoBERTClassifier`:
> - `__init__(self, model_path: str)`: load pipeline v·ªõi `local_files_only=True`
> - `predict(self, text: str) -> dict`:
>   - N·∫øu text r·ªóng ‚Üí tr·∫£ `{'label': 'neutral', 'score': 1.0}`
>   - G·ªçi pipeline ‚Üí tr·∫£ `{'label': str, 'score': float}`
>   - Ch·ªâ ch·∫•p nh·∫≠n 4 label: `positive|negative|neutral|toxic`"

### ‚úÖ Prompt 2.4 ‚Äì WebSocket endpoint real-time
> "Trong `backend/app/api/v1/endpoints.py`:
> - Vi·∫øt endpoint `websocket_endpoint(websocket: WebSocket)`:
>   1. `await websocket.accept()`
>   2. V√≤ng l·∫∑p `while True`:
>      - `data = await websocket.receive_bytes()`
>      - G·ªçi `audio_processor.process(data)` ‚Üí tr·∫£ `TranscriptResult`
>      - `await websocket.send_json(result.dict())`
>   3. B·∫Øt exception ‚Üí log l·ªói, kh√¥ng crash
> - ƒêƒÉng k√Ω route: `@app.websocket('/v1/ws')` trong `main.py`"

---

## üîπ GIAI ƒêO·∫†N 3: FRONTEND ‚Äì REAL-TIME UI & WEBSOCKET

### ‚úÖ Prompt 3.1 ‚Äì C·∫•u h√¨nh d·ª± √°n React + Vite
> "T·∫°o th∆∞ m·ª•c `frontend/` v·ªõi:
> ```
> frontend/
> ‚îú‚îÄ‚îÄ src/
> ‚îÇ   ‚îú‚îÄ‚îÄ app/page.tsx
> ‚îÇ   ‚îú‚îÄ‚îÄ hooks/
> ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useAudioRecorder.ts
> ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ useWebSocket.ts
> ‚îÇ   ‚îú‚îÄ‚îÄ store/useStore.ts     # Zustand
> ‚îÇ   ‚îú‚îÄ‚îÄ components/
> ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TranscriptDisplay.tsx
> ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ RecordingButton.tsx
> ‚îÇ   ‚îî‚îÄ‚îÄ types/index.ts        # TranscriptResult
> ‚îú‚îÄ‚îÄ tailwind.config.ts
> ‚îî‚îÄ‚îÄ package.json
> ```
> - C√†i: `react`, `vite`, `zustand`, `wavesurfer.js`, `@radix-ui/react-*`, `shadcn-ui`
> - Kh√¥ng c√†i ƒë·∫∑t `axios` (ch·ªâ d√πng WebSocket)"

### ‚ö†Ô∏è FE: Zod v4 + folder-pattern rules (MANDATORY)

- Th√™m dependency `zod` (v4) v√† `@hookform/resolvers` v√†o `frontend/package.json`.
- S·ª≠ d·ª•ng Zod v4 cho runtime schema validation v√† infer types cho TypeScript.
- Pattern b·∫Øt bu·ªôc v·ªÅ file/folder:
	- `src/schemas/` ‚Üí m·ªói file ch·ª©a 1 Zod schema (v√≠ d·ª• `transcript.schema.ts` exporting `TranscriptSchema`).
	- `src/types/` ho·∫∑c `src/interfaces/` ‚Üí m·ªói file ch·ª©a 1 TypeScript type/interface. Kh√¥ng ƒë·ªãnh nghƒ©a types trong c√πng file v·ªõi schema. V√≠ d·ª•: `src/types/transcript.ts` c√≥ `export type Transcript = z.infer<typeof TranscriptSchema>`.
	- Kh√¥ng gom nhi·ªÅu schema/type v√†o m·ªôt file.

### üîó React Hook Form + Zod (recommended integration)

- Install: `npm i react-hook-form @hookform/resolvers zod`
- Usage pattern:
	- define schema in `src/schemas/transcript.schema.ts`:
		```ts
		import { z } from 'zod'
		export const TranscriptSchema = z.object({
			text: z.string(),
			label: z.enum(['positive','negative','neutral','toxic']),
			confidence: z.number().min(0).max(1),
			timestamp: z.number().optional()
		})
		```
	- infer types in `src/types/transcript.ts`:
		```ts
		import { z } from 'zod'
		import { TranscriptSchema } from '../schemas/transcript.schema'
		export type Transcript = z.infer<typeof TranscriptSchema>
		```
	- use in component with react-hook-form:
		```ts
		const form = useForm({ resolver: zodResolver(TranscriptSchema) })
		```

### üé® Shadcn UI guidance (templates & components)

- Use official site `https://ui.shadcn.com/docs` to pick components and templates.
- Recommended components for this project: `Badge`, `Alert`, `Toast`, `List`, `Card`, `Button`, `Form` components.
- Keep UI wrappers in `src/components/ui/` so you can swap underlying implementations easily.

### üîé Research rule for AI agent

- BEFORE making FE changes, the AI agent must research and cite official docs (Zod v4 API, React Hook Form + Zod resolver, Shadcn UI components) in the commit/PR description. Include minimal usage snippets where helpful.

### ‚úÖ Prompt 3.2 ‚Äì Hook ghi √¢m real-time
> "Vi·∫øt `useAudioRecorder.ts`:
> - Tr·∫£ v·ªÅ: `{ isRecording: boolean, startRecording: () => void, stopRecording: () => void }`
> - B√™n trong:
>   - `useRef<MediaRecorder | null>(null)`
>   - `navigator.mediaDevices.getUserMedia({ audio: true })`
>   - `new MediaRecorder(stream, { mimeType: 'audio/webm;codecs=opus' })`
>   - `ondataavailable`: g·ª≠i `e.data.arrayBuffer()` qua callback `onChunk`
>   - `onstop`: cleanup
> - Chunk interval: 2000ms"

### ‚úÖ Prompt 3.3 ‚Äì Hook WebSocket client
> "Vi·∫øt `useWebSocket.ts`:
> - Nh·∫≠n `onMessage: (data: TranscriptResult) => void`
> - T·ª± ƒë·ªông connect ƒë·∫øn `ws://localhost:8000/v1/ws`
> - Ph∆∞∆°ng th·ª©c `sendAudioChunk(chunk: ArrayBuffer)`
> - X·ª≠ l√Ω reconnect (3 l·∫ßn, delay 1s)
> - D√πng `useRef<WebSocket>` ƒë·ªÉ tr√°nh re-create"

### ‚úÖ Prompt 3.4 ‚Äì UI hi·ªÉn th·ªã real-time
> "T·∫°o component `TranscriptDisplay.tsx`:
> - D√πng Zustand store: `transcripts: { id: string; text: string; label: string; warning: boolean }[]`
> - Map qua m·∫£ng ‚Üí render t·ª´ng d√≤ng
> - N·∫øu `warning: true`:
>   - D√πng `<Badge variant='destructive'>{label}</Badge>`
>   - D√≤ng text c√≥ n·ªÅn `bg-red-50/20`
> - Cu·ªôn t·ª± ƒë·ªông xu·ªëng cu·ªëi khi c√≥ d√≤ng m·ªõi"

---

## üîπ GIAI ƒêO·∫†N 4: INTEGRATION & VALIDATION

### ‚úÖ Prompt 4.1 ‚Äì Test end-to-end real-time
> "Vi·∫øt h∆∞·ªõng d·∫´n test:
> 1. Ch·∫°y BE: `cd backend && uvicorn app.main:app --reload --port 8000`
> 2. Ch·∫°y FE: `cd frontend && npm run dev`
> 3. M·ªü `http://localhost:5173`
> 4. Nh·∫•n 'Start Recording'
> 5. N√≥i: 'ƒê·ªì ng·ªëc, m√†y th·∫≠t l√† t·ªá!'
> 6. Ki·ªÉm tra:
>    - FE hi·ªÉn th·ªã text ph·∫£i ƒë√∫ng
>    - C√≥ badge 'toxic' m√†u ƒë·ªè
>    - Latency < 2.5s
> 7. Log BE kh√¥ng c√≥ l·ªói decode"

### ‚úÖ Prompt 4.2 ‚Äì Vi·∫øt README.md chu·∫©n dev Vi·ªát
> "T·∫°o `README.md` ·ªü root:
> - Ti√™u ƒë·ªÅ: 'Demo Local: Speech-to-Text + Ph√°t hi·ªán n·ªôi dung ƒë·ªôc h·∫°i (Ti·∫øng Vi·ªát)'
> - M·ª•c 'Y√™u c·∫ßu':
>   - Python 3.11+, Node 22+
>   - ƒê√£ clone 2 model v√†o `./models/`
> - M·ª•c 'C√°ch ch·∫°y':
>   ```bash
>   # Backend
>   cd backend
>   pip install -r requirements.txt
>   uvicorn app.main:app --port 8000
> 
>   # Frontend
>   cd frontend
>   npm install
>   npm run dev
>   ```
> - M·ª•c 'License':
>   - 'M√¥ h√¨nh ASR: CC BY-NC 4.0 ‚Äì ch·ªâ d√πng phi th∆∞∆°ng m·∫°i'
>   - Tr√≠ch d·∫´n Zenodo v√† email t√°c gi·∫£ PhoBERT
> - M·ª•c 'Demo': ·∫£nh minh h·ªça (placeholder)"

---

## üö´ C·∫§M
- G·ªçi Hugging Face Hub
- D√πng librosa (ph·∫£i d√πng torchaudio)
- D√πng Axios ho·∫∑c REST API

## ‚úÖ HO√ÄN TH√ÄNH KHI
- [ ] Ch·∫°y local kh√¥ng c·∫ßn internet
- [ ] N√≥i ‚Üí ra text ‚Üí ph√°t hi·ªán 'toxic'
- [ ] C√≥ test ƒë∆°n v·ªã (BE + FE)
- [ ] C√≥ README r√µ r√†ng, tr√≠ch d·∫´n ƒë·∫ßy ƒë·ªß